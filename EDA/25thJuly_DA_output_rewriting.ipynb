{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92109970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"tags\": [\"hide-input\",]}\n",
    "\n",
    "mainfile = \"result_2023_6_8.json\"\n",
    "maindata = []\n",
    "with open(mainfile, 'r') as f:\n",
    "  for entry in f:\n",
    "      oneEntry = json.loads(entry)\n",
    "      maindata.append(oneEntry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326f622",
   "metadata": {},
   "source": [
    "# Data Analysis for JSON Files (draft)\n",
    "Looking into website leak data from edutech extension. Written up version of exploratory data analysis. <br>\n",
    "**Mentor:** Jake Chanesson,   **Research Assistant:** Shayona Basu\n",
    "\n",
    "Data produced by **AirLAB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a2cd4",
   "metadata": {},
   "source": [
    "### Removing data entries where the json file has no report data saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "for d in maindata: \n",
    "  if d.get('report') != None: #not empty\n",
    "    if d.get('report').get('user_id') != None: \n",
    "      filtered_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd52121",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"After the initial filtiration there are {len(filtered_data)} entries, when there were initially {len(maindata)} entries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52e7db",
   "metadata": {},
   "source": [
    "I am curious to check what actually consists of empty data. I am defining empty data as data where reports are made, but there is no user_id. I am going to go through and explore what the empty data consists of. I believe this can be useful to understand we are not using, and confirming it is empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = []\n",
    "for d in maindata: \n",
    "    if d.get('report') != None: \n",
    "        if d.get('report').get('user_id') == None:\n",
    "            empty_data.append(d)\n",
    "print(f\"There are {len(empty_data)} entries of json data with reports but no user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f33488",
   "metadata": {},
   "source": [
    "Printing a random entry of the empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86384203",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data[38473]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b993c76",
   "metadata": {},
   "source": [
    "From checking this random entry, we see there is seemingly useful amount of information, so I think we should edit how our first stage of filtering, and potentially include reports where there is no user_id. We can save the data with no user_id, in its own group and continue to do website analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Of all the data entries from the files, the initial filtration removes {round(len(empty_data)/len(maindata) *100,2)}%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data1 = []\n",
    "for d in maindata: \n",
    "  if d.get('report') != None: #not empty\n",
    "    if d.get('report').get('initiator_domain') != None: \n",
    "        filtered_data1.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be18ca",
   "metadata": {},
   "source": [
    "I think maybe a better initial filtering would be to remove data where the initiator domain data is not collected. I think now is a good time to define two important cateogires for websites collected, throughout the rest of this data analysis <br>\n",
    "**Initiator Domain:**   The website where the user lands, where the leak then occurs. \n",
    "**Tracker URL:**   The website of which the tracker owns, and where the leak actually happens\n",
    "\n",
    "Documentation: https://homes.esat.kuleuven.be/~asenol/leaky-forms/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a47537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(filtered_data1)} websites that have the leak origin website data. Accordingly, there were {len(maindata)-len(filtered_data1)} entries, or {round(((len(maindata)-len(filtered_data1))/len(maindata))*100,2)}% removed in this updated initial filtration.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcef09c",
   "metadata": {},
   "source": [
    "### Here is an example entry of the new filtered dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data1[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cee93",
   "metadata": {},
   "source": [
    "Now, I will begin to make sense of reduced dataset, `filtered_data1`. As per the definitions a few cells above, I will now go through the filtered data set, and collect the Initiator URL and the Leak URL, and present it in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aabcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Leak_URLS and Origin_URLS\n",
    "Leak_URLS = []\n",
    "Origin_URLS = []\n",
    "for d in filtered_data1: \n",
    "    rep = d['report']\n",
    "    Origin_URLS.append(rep['initiator_domain'])\n",
    "    if rep.get('tracker_info') != None: \n",
    "        #some reports do not have tracker info\n",
    "        if rep['tracker_info'].get('tracker').get('owner').get('url') != None: \n",
    "            #all reports with tracker info, have the following\n",
    "            Leak_URLS.append(d['report']['tracker_info']['tracker']['owner']['url'])\n",
    "        else: Leak_URLS.append(\"None\")\n",
    "    else: Leak_URLS.append(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee717e",
   "metadata": {},
   "source": [
    "Since writing data into a Pandas Dataframe requires both columns to be of the same length, I have to ammend my code a bit, by adding the string \"None\" when there is not a Leak_URl recorded. Here I am just ensuring both lists, which will be inputted as future column data, are the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47257c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Leak_URLS), len(Origin_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66367f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leak_origin_urls = pd.DataFrame.from_dict({'Leak URLS': Leak_URLS, 'Origin URLS': Origin_URLS})\n",
    "df_leak_origin_urls.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1faee",
   "metadata": {},
   "source": [
    "This is interesting, but presenting the data in this way is not letting us see a clear picture of what is happening. I want to find a way to get more useful data from this. First, let me start with seeing more of the origin urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(set(Origin_URLS))} unique origin urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99910de",
   "metadata": {},
   "source": [
    "Writing a simple function that goes through a list and produces a dictionairy that counts how many times that entry appears, and sorts it in ascending order. I will then pass origin_urls through, and see what are the top websites where leaks occur from. The function returns it as a dictionairy, but I will present it as a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_produce_count(list):\n",
    "    count_d = {}\n",
    "    for i in list: \n",
    "        if i in count_d: \n",
    "            a = count_d.get(i)\n",
    "            count_d[i] = a + 1\n",
    "        else: \n",
    "            count_d[i] = 1\n",
    "            \n",
    "    return dict(sorted(count_d.items(), key=lambda x:x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_urls = simple_produce_count(Origin_URLS)\n",
    "df_leak_origin_urls = pd.DataFrame.from_dict({'Origin of Leak Website URLS': list(orig_urls.keys()), 'Appearing': list(orig_urls.values())})\n",
    "df_leak_origin_urls['Proportion (%)'] = round((df_leak_origin_urls.Appearing/len(Leak_URLS))*100,2)\n",
    "df_leak_origin_urls.rename_axis(\"Count\").head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9969e1b",
   "metadata": {},
   "source": [
    "*In the above table, I displayed the top 30 websites from the total 65 leak websites found in the data, for consideration of space. I will allow all tables to be presented in full format in the appendix*  <br>\n",
    "\n",
    "#### Analysis on Google Docs being the source of the most leaks\n",
    "So, we see that 38.2% of the leaks originate from Google Docs. It is also relevant to note that Google Docs is heavily used by students to write classwork and class projects. Furthur, the Google enviroment is a popular system that schools integrate into learning, and students store most of their school or personal related information. Therefore leaks relating to this would be of significance to educator's who care about student's online security. <br>\n",
    "\n",
    "We see google.com, the search platform being responsible for 17.18% of the leaks, or 60,847 leaked data from students. Google Mail takes up 5.38% of the leaks, and Google Calender takes up 2.16%. The last significant source of leaks from the Google ecosystem is Google Calendar which has 2.16%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecd687",
   "metadata": {},
   "source": [
    "### What leak websites collect the leak data from the origin websites?\n",
    "A next step in this analysis could be to connect what leak websites collect data from these websites. It would be interesting to see if there is a relationship from websites where the leak is taking place, and what the tracker collects is of the leak website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedfd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
