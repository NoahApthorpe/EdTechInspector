{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2909911",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainfile = \"result_2023_6_8.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef155c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maindata = []\n",
    "with open(mainfile, 'r') as f:\n",
    "    for entry in f:\n",
    "        oneEntry = json.loads(entry)\n",
    "        maindata.append(oneEntry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf3ac3",
   "metadata": {},
   "source": [
    "Removing any redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b45a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtereddata = []\n",
    "for d in maindata:\n",
    "    if d.get('report') != None:\n",
    "        if d.get('report').get('user_id') != None:\n",
    "            filtereddata.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25199294",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(maindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3316eb",
   "metadata": {},
   "source": [
    "Removing all non-GET request entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01f9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getrequests = []\n",
    "for d in filtereddata:\n",
    "    if d.get('report').get('request_method') == 'GET':      \n",
    "        getrequests.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(maindata))\n",
    "print(len(filtereddata))\n",
    "print(len(getrequests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8842b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "users = {}\n",
    "\n",
    "emptydata = {\n",
    "    \"lastname\":0,\n",
    "    \"firstname\":0,\n",
    "    \"preferredname\":0,\n",
    "    \"id\":0,\n",
    "    \"email\":0,\n",
    "    \"phone\":0,\n",
    "}\n",
    "\n",
    "\n",
    "URLs = {}\n",
    "\n",
    "\n",
    "user_data = {\n",
    "    \"URL\":copy.deepcopy(emptydata),\n",
    "    \"body\":copy.deepcopy(emptydata),\n",
    "    \"URLs\":copy.deepcopy(URLs)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416e919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in getrequests:\n",
    "    user_id = d.get('report').get('user_id')\n",
    "    if user_id not in users:\n",
    "        users[user_id] = copy.deepcopy(user_data)\n",
    "        print(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in getrequests:\n",
    "    user_id = d.get('report').get('user_id')\n",
    "    url_leak_type = d.get('report').get('url_leak_type')\n",
    "    body_leak_type = d.get('report').get('body_leak_type')\n",
    "    domain = d.get('report').get('initiator_domain')\n",
    "    \n",
    "    \n",
    "    if url_leak_type != None:\n",
    "        for e in url_leak_type:\n",
    "            users[user_id][\"URL\"][e] = users[user_id][\"URL\"][e] + 1\n",
    "    \n",
    "    if body_leak_type != None:\n",
    "        for e in body_leak_type:\n",
    "            users[user_id][\"body\"][e] = users[user_id][\"body\"][e] + 1\n",
    "    \n",
    "    if domain != None:\n",
    "        if domain in users[user_id][\"URLs\"]:\n",
    "            users[user_id][\"URLs\"][domain] += 1\n",
    "        else:\n",
    "            users[user_id][\"URLs\"][domain] = 1          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b35eb",
   "metadata": {},
   "source": [
    "Just turning the above data into a nice table using tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a992d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = []\n",
    "for u in users:\n",
    "    entry = []\n",
    "    entry.append(u)\n",
    "    for e in users.get(u).get(\"URL\"):\n",
    "        entry.append(users.get(u).get(\"URL\").get(e))\n",
    "    \n",
    "    data.append(entry)\n",
    "\n",
    "headers = ['USER ID', 'Last Name', 'First Name','Pref. Name', 'ID', 'Email', \"Phone #\"]\n",
    "table = tabulate(data, headers=headers, tablefmt='grid')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca1802",
   "metadata": {},
   "source": [
    "Evidently, users \"dknglbh1h3\" and \"zu7pjmkeoi\" are outliers. Interestingly, this data doesn't quite line up with some older calculations for the number of leaks of each type, though those calculations probably didn't use the same filtering methodology I did. Everything's on the same order of magnitude, so it doesn't look like we have any problems. I've excluded those two users from the data set for the later analyses.\n",
    "\n",
    "I'm also only showing data from the url_leak_type field, and not the body_leak_type field, even though I collected data from both. As it turns out, no personal information was found in the body_leak_type field. Possibly just encrypted, not absent, though. See below for the dictionaries.\n",
    "\n",
    "(For what it's worth, rerunning this notebook from the top shuffles the order the USER IDs are presented in. Not sure why, but something to note down, I suppose.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c9421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for u in users:\n",
    "    print(u + \"\\t\" + str(users.get(u).get(\"body\")) )\n",
    "    \n",
    "for u in users:\n",
    "    print(u + \"\\t\" + str(users.get(u).get(\"URLs\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeddata = []\n",
    "\n",
    "for d in getrequests: \n",
    "    if d.get('report').get('user_id') != \"dknglbh1h3\" and d.get('report').get('user_id') != \"zu7pjmkeoi\":\n",
    "        normalizeddata.append(d)\n",
    "\n",
    "print(len(normalizeddata))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c33908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaksOnlyNormalized = []\n",
    "\n",
    "for d in normalizeddata: \n",
    "    if d.get('report').get('initiator_domain') != None:\n",
    "        leaksOnlyNormalized.append(d)\n",
    "        \n",
    "print(len(leaksOnlyNormalized))            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e18b0",
   "metadata": {},
   "source": [
    "The number of entries was reduced by 114,411 via filtering out of the two outlying users. There are 114,021 entries in which dknglbh1h3 or zu7pjmkeoi had a last name leak. Numbers seemingly track-- those two users had their last name \"leaked\" with nearly every search.\n",
    "\n",
    "Filtering out entries without an initiator domain, we find that there are only 592 entries (down from the initial >100,000 leaked data entries detected-- so evidently those users were drastically skewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "sites = {}\n",
    "\n",
    "leakCount = {\n",
    "    \"lastname\":0,\n",
    "    \"firstname\":0,\n",
    "    \"preferredname\":0,\n",
    "    \"id\":0,\n",
    "    \"email\":0,\n",
    "    \"phone\":0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae87443",
   "metadata": {},
   "outputs": [],
   "source": [
    "initiatorDomains = []\n",
    "\n",
    "for d in leaksOnlyNormalized:\n",
    "    domain = d.get('report').get('initiator_domain')\n",
    "    if domain not in sites:\n",
    "        sites[domain] = copy.deepcopy(leakCount)\n",
    "        print(domain)\n",
    "        initiatorDomains.append(domain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb05dcf",
   "metadata": {},
   "source": [
    "The list of initiator domains is worryingly short? Don't know if this pilot dataset is just that small or if something else is going on. Maybe an oversight on my part, but might just also mean we need a bigger dataset to make any more meaningful conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in leaksOnlyNormalized:\n",
    "    domain = d.get('report').get('initiator_domain')\n",
    "    url_leak_type = d.get('report').get('url_leak_type')\n",
    "    \n",
    "    if url_leak_type != None:\n",
    "        for e in url_leak_type:\n",
    "            sites[domain][e] = sites[domain][e] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc1060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in sites:\n",
    "    entry = []\n",
    "    entry.append(d)\n",
    "    for e in sites.get(d):\n",
    "        entry.append(sites.get(d).get(e))\n",
    "    \n",
    "    data.append(entry)\n",
    "\n",
    "headers = ['DOMAIN', 'Last Name', 'First Name','Pref. Name', 'ID', 'Email', \"Phone #\"]\n",
    "table = tabulate(data, headers=headers, tablefmt='grid')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c1091",
   "metadata": {},
   "source": [
    "This has already been said, but with internal trackers, I doubt this is the full extent of what data collection is occurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46625171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def shorten(url):\n",
    "    # Parse the URL into its components\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Reconstruct the URL with only the scheme, netloc, and fragment\n",
    "    new_url = urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', parsed_url.fragment))\n",
    "\n",
    "    return new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493b72b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "domainsAndLeaks = {}\n",
    "\n",
    "\n",
    "for mainDomain in initiatorDomains:\n",
    "    \n",
    "    leakURLs = {}\n",
    "    \n",
    "    for d in leaksOnlyNormalized:\n",
    "        domain = d.get('report').get('initiator_domain')\n",
    "        if domain == mainDomain:\n",
    "            leakURL = d.get('report').get('leak_url')\n",
    "            leakURL = shorten(leakURL)\n",
    "        \n",
    "            if leakURL not in leakURLs:\n",
    "                leakURLs[leakURL] = 1\n",
    "            else:\n",
    "                leakURLs[leakURL] += 1\n",
    "                \n",
    "    domainsAndLeaks[mainDomain] = leakURLs\n",
    "    \n",
    "print(domainsAndLeaks[\"https://www.google.com/\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1a0ae",
   "metadata": {},
   "source": [
    "Some weird quirks here: for example, some of the URLs seem to be broken & including \"LEAKED_FIRSTNAME\" in places where it clearly shouldn't be. There's also an entire URL that seems to be wiped clean (I suspect that was marked Null in the original dataset, though how there's a leak without a Leak URL probably also warrants investigation). It might be due to the package I'm using to shorten the URLs, but I suspect it's more likely a relic of the scanner that needs to be fixed: maybe it should only be looking for leaks after the top level domain? Warrants discussion-- so for now I'll leave the error intact instead of manually paving over it until the cause is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e005cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in initiatorDomains:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd1013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "value = input(\"Please request an initiator domain from the list above, or type 'exit' to close: \")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    if value == \"exit\":\n",
    "        break\n",
    "    data = domainsAndLeaks[value]\n",
    "    table_data = [[key, value] for key, value in data.items()]\n",
    "\n",
    "    table = tabulate(table_data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")\n",
    "    print(table)\n",
    "    print(\"\\n\\n\")\n",
    "    value = input(\"Please request an initiator domain, or type 'exit' to close: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb7f87",
   "metadata": {},
   "source": [
    "This is a clunky little piece of code, and in future versions it might be better to just print everything, but I find this is a good way to make it easy to compare two specific URLs and keep the view short and sweet by focusing on what we determine relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966f934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
